<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Data Engineering the Left</title><link href="https://austinweisgrau.github.io/" rel="alternate"></link><link href="https://austinweisgrau.github.io/feeds/all.atom.xml" rel="self"></link><id>https://austinweisgrau.github.io/</id><updated>2024-10-29T00:00:00-07:00</updated><entry><title>Tooling for Big Data Transformations</title><link href="https://austinweisgrau.github.io/tooling-for-big-data-transformations.html" rel="alternate"></link><published>2024-10-29T00:00:00-07:00</published><updated>2024-10-29T00:00:00-07:00</updated><author><name>Austin Weisgrau</name></author><id>tag:austinweisgrau.github.io,2024-10-29:/tooling-for-big-data-transformations.html</id><summary type="html">&lt;p&gt;Data teams sometimes struggle to find appropriate tooling for large
analytics workflows. A common pattern is for an analyst to work
primarily using the R tidyverse or Python pandas. This works fine for
awhile, but as the workflow scales up and the amount of data being
processed increases, eventually these …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Data teams sometimes struggle to find appropriate tooling for large
analytics workflows. A common pattern is for an analyst to work
primarily using the R tidyverse or Python pandas. This works fine for
awhile, but as the workflow scales up and the amount of data being
processed increases, eventually these tools struggle to keep up. Even
running these workflows on cloud servers with massive amounts of &lt;span class="caps"&gt;CPU&lt;/span&gt;
and &lt;span class="caps"&gt;RAM&lt;/span&gt; doesn&amp;#8217;t scale&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;In my opinion, the only truly appropriate tooling for data
transformations above a very small scale is to load the data into an
&lt;span class="caps"&gt;OLAP&lt;/span&gt; data warehouse and execute your transformations using &lt;span class="caps"&gt;SQL&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, sometimes you need a drop-in solution that can scale-up
dataframe-based&amp;nbsp;transformations. &lt;/p&gt;
&lt;p&gt;Traditionally, the usual approach was to use distributed computing
frameworks like Spark or Hadoop to execute dataframe transformations
at scale. However, these require pretty elaborate and complex cloud&amp;nbsp;infrastructure.&lt;/p&gt;
&lt;p&gt;These days, there are much better tools for working with big data on a
single machine, using columnar data storage locally like an &lt;span class="caps"&gt;OLAP&lt;/span&gt;&amp;nbsp;database. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Apache Arrow is a &lt;span class="caps"&gt;CSV&lt;/span&gt;-alternative typed columnar data storage format
  that enables pandas-alternatives like &lt;a href="https://github.com/pola-rs/polars"&gt;polars&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;duckdb is a sqlite-like dead simple local database that uses
  columnar storage behind the scenes, and can be used as a backend for
  &lt;a href="https://duckdb.org/docs/api/python/spark_api"&gt;Spark&lt;/a&gt;&amp;nbsp;workflows&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I &lt;a href="https://austinweisgrau.github.io/deduplicating-everyaction.html"&gt;previously wrote&lt;/a&gt; about how at Working Families Party we use duckdb
as the backend for a machine learning identity resolution workflow
(&lt;a href="https://moj-analytical-services.github.io/splink/getting_started.html"&gt;splink&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;For an overview of the larger trends in cloud computing and big data
transformations, see this really good article by Mother Duck, the
company behind DuckDB: &lt;a href="https://motherduck.com/blog/the-simple-joys-of-scaling-up/"&gt;The Simple Joys of Scaling&amp;nbsp;Up&lt;/a&gt;&lt;/p&gt;</content><category term="learning"></category></entry><entry><title>Deduplicating EveryAction</title><link href="https://austinweisgrau.github.io/deduplicating-everyaction.html" rel="alternate"></link><published>2024-07-02T00:00:00-07:00</published><updated>2024-07-02T00:00:00-07:00</updated><author><name>Austin Weisgrau</name></author><id>tag:austinweisgrau.github.io,2024-07-02:/deduplicating-everyaction.html</id><summary type="html">&lt;p&gt;Like most progressive political organizations, Working Families Party
(&lt;span class="caps"&gt;WFP&lt;/span&gt;) reluctantly uses EveryAction, the volunteer-management side of
&lt;span class="caps"&gt;NGPVAN&lt;/span&gt;, as our &lt;span class="caps"&gt;CRM&lt;/span&gt; for keeping track of our activists and
donors. EveryAction has many issues, support is difficult to access,
and its issues are unlikely to dramatically improve since its recent
acquisition by …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Like most progressive political organizations, Working Families Party
(&lt;span class="caps"&gt;WFP&lt;/span&gt;) reluctantly uses EveryAction, the volunteer-management side of
&lt;span class="caps"&gt;NGPVAN&lt;/span&gt;, as our &lt;span class="caps"&gt;CRM&lt;/span&gt; for keeping track of our activists and
donors. EveryAction has many issues, support is difficult to access,
and its issues are unlikely to dramatically improve since its recent
acquisition by private equity and subsequent mass&amp;nbsp;layoffs.&lt;/p&gt;
&lt;p&gt;One common issue with EveryAction is the proliferation of duplicate
contacts. Staff working with EveryAction will search for a contact and
notice that two or more contacts for the same person seem to
exist. This creates a problem with getting accurate metrics out of
EveryAction and also introduces confusing workflow issues for any
staff who want to add or update contact attributes. Which contact
should be updated? Pick either one? Update&amp;nbsp;both?&lt;/p&gt;
&lt;p&gt;Contacts can be manually merged in the EveryAction web app, but the
process is slow and untenable if duplicates exist at scale. And for
any organization that uses EveryAction for several years or longer,
duplicates can certainly go to scale. At &lt;span class="caps"&gt;WFP&lt;/span&gt; we estimate that
somewhere between 5%-10% of our EveryAction contacts are&amp;nbsp;duplicates.&lt;/p&gt;
&lt;p&gt;Bonterra, the organization behind EveryAction, is aware of the common
issue with duplication and offers a paid service to deduplicate your
data for you. The fee for the service is exorbitant and a questionable
offering when dealing with duplicates is arguably more of a bug in the
product than a feature to be upsold&amp;nbsp;on.&lt;/p&gt;
&lt;p&gt;We decided to tackle deduplicating our data ourselves and want to
share our process and learnings with the rest of the progressive
organizations who are still locked in to this&amp;nbsp;product. &lt;/p&gt;
&lt;p&gt;To tackle the proliferation of duplicates in EveryAction, we needed to
answer a few key&amp;nbsp;questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Where are the duplicates coming from? // How can we mitigate the
   creation of new&amp;nbsp;duplicates?&lt;/li&gt;
&lt;li&gt;Which contacts are duplicates? // How can we match and merge
   existing&amp;nbsp;duplicates?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Whence&amp;nbsp;??&lt;/h2&gt;
&lt;h3&gt;Where are the duplicates coming&amp;nbsp;from?&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Before we dive in, I will note that I did this engineering work and wrote this blog post about a year ago, and some small things have changed with EveryAction’s duplicate handling in that time. Some of the details I found through trial and error may not still behave in exactly the same way, although most of the behavior described here should still&amp;nbsp;hold.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When data is loaded into EveryAction, EveryAction attempts to validate the new data with an existing contact record using one of four combinations of data&amp;nbsp;fields.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first name, last name, date of&amp;nbsp;birth&lt;/li&gt;
&lt;li&gt;first name, last name, phone&amp;nbsp;number&lt;/li&gt;
&lt;li&gt;first name, last name,&amp;nbsp;email&lt;/li&gt;
&lt;li&gt;first name, last name, street number, street name,&amp;nbsp;zipcode&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If a match is found on any of those sets of attributes, EveryAction
will load associated data to that existing matched contact. Good! If
no match can be found, a new contact is created. Makes&amp;nbsp;sense.&lt;/p&gt;
&lt;p&gt;There are a few bugs in this process, however, that cause issues with&amp;nbsp;matching.&lt;/p&gt;
&lt;p&gt;If a phone or email that can&amp;#8217;t be parsed as valid are provided in the
set of contact attributes, EveryAction will fail to make a match, even
if there are enough other attributes available to make a&amp;nbsp;match.&lt;/p&gt;
&lt;p&gt;If none of the preferred supplementary contact attributes are
available, EveryAction will make a new contact. If you only ever
obtain first names and last names for certain contacts and regularly
load that data into EveryAction, you will make duplicates of those
same contacts each and every&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;And of course, people&amp;#8217;s contact details and even their names change
over time, typos arise all the time,&amp;nbsp;etc.&lt;/p&gt;
&lt;h3&gt;How can we mitigate the creation of new&amp;nbsp;duplicates?&lt;/h3&gt;
&lt;p&gt;To mitigate the creation of new duplicates, we needed to improve the
performance of the &amp;#8220;matching&amp;#8221; step that happens before data is loaded
onto a matched or created&amp;nbsp;contact.&lt;/p&gt;
&lt;p&gt;Data is loaded into our EveryAction instance in three&amp;nbsp;ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Some tools like Mobilize have built-in integrations with
   EveryAction and load data&amp;nbsp;directly.&lt;/li&gt;
&lt;li&gt;We use the &lt;span class="caps"&gt;API&lt;/span&gt; to programmatically sync data from other
   platforms (like ActBlue and Spoke) that don&amp;#8217;t have direct&amp;nbsp;integrations.&lt;/li&gt;
&lt;li&gt;Sometimes staff do a bulk upload manually through the web&amp;nbsp;app. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I assume that all three methods use the same logic to make matches,
but I&amp;#8217;ve only personally verified the behavior of the &lt;span class="caps"&gt;API&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The EveryAction &lt;span class="caps"&gt;API&lt;/span&gt; offers two endpoints for finding contact matches.
- &lt;code&gt;/find&lt;/code&gt; will search for a match, and raise various errors on failure
  to find a match.
- &lt;code&gt;/findOrCreate&lt;/code&gt; will search for a match, and create a new contact if
  no match is found. This endpoint always returns a contact, and
  there&amp;#8217;s no simple way to know if the contact was a match or is a new&amp;nbsp;creation.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;/findOrCreate&lt;/code&gt; is very convenient for getting a sync up and
running. Many of our legacy syncs to EveryAction used that
endpoint. However, to mitigate the creation of new duplicates, we need
to improve the default matching behavior. We can achieve this
improvement by separating the &amp;#8220;find&amp;#8221; and &amp;#8220;create&amp;#8221; steps, and adding
more flexibility into the matching logic before reverting to new contact&amp;nbsp;creation.&lt;/p&gt;
&lt;p&gt;In pseudo-code, this looks like moving&amp;nbsp;from:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;contact_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;van_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find_or_create_van_id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;contact_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;to&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;contact_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;van_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find_van_id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;contact_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;van_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;van_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;try_harder_to_find_van_id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;contact_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;van_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;van_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_van_id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;contact_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If we don&amp;#8217;t default to creating a new contact whenever EveryAction
fails to find a match, we have the opportunity to add some more
flexible and intelligent matching logic into the&amp;nbsp;process.&lt;/p&gt;
&lt;p&gt;There are several strategies we&amp;#8217;ve developed that seem to make many
reliable matches that EveryAction fails to make. We are continuing to
monitor the creation of new contacts to identify other heuristics that
can be used in a function like &lt;code&gt;try_harder_to_find_van_id()&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If a middle initial or middle name gets input as part of a
   first name, EveryAction will fail to make a match. e.g. &amp;#8220;Barbara
   E. Walters, 858-555-5555&amp;#8221; will not match to &amp;#8220;Barbara Walters,
   858-555-5555&amp;#8221;. Removing characters from a first name after an
   initial space can more reliably match&amp;nbsp;contacts. &lt;/li&gt;
&lt;li&gt;Apostrophes and quote marks are not reliably parsed by source
   platforms or EveryAction. e.g. &amp;#8220;Mark O&amp;#8217;Donnell, 619-555-5555&amp;#8221;
   will not match with &amp;#8220;Mark ODonnell, 619-555-5555&amp;#8221; or &amp;#8220;Mark
   O`Donnell, 619-555-5555&amp;#8221;. Removing apostrophes and quote marks from
   names seems to improve&amp;nbsp;matching.&lt;/li&gt;
&lt;li&gt;Cleaning and validating phone numbers, emails, and street numbers
   and zipcodes before attempting to make a match improves matching.
   e.g. &amp;#8220;John Green, 750-555-3499a, jgreen @hotmail.co&amp;#8221; will not
   normally match to &amp;#8220;John Green, 750-555-3499,&amp;nbsp;jgreen@hotmail.com&amp;#8221;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A working example version of a python function built on top of the
 &lt;code&gt;parsons&lt;/code&gt; package that implements the logic described above can be
 seen&amp;nbsp;here.&lt;/p&gt;
&lt;p&gt;By cleaning and validating data and making other improvements to
matching heuristics, we can more reliably find matches to our source
data before defaulting to creating new&amp;nbsp;contacts.&lt;/p&gt;
&lt;h2&gt;Whomst&amp;nbsp;??&lt;/h2&gt;
&lt;h3&gt;Which contacts are&amp;nbsp;duplicates?&lt;/h3&gt;
&lt;p&gt;The problem of identifying records in a dataset that refer to the same
person is called &amp;#8220;entity resolution,&amp;#8221; or more specifically &amp;#8220;identity
resolution&amp;#8221; if your duplicated entities are&amp;nbsp;people.&lt;/p&gt;
&lt;p&gt;Identity resolution is a nearly universal data quality
issue. Depending on how data is gathered, it is usually very hard or
impossible to track individuals over time with complete
certainty. There are very few contact attributes that are not subject
to change over time. In the context of community organizing, we only
generally gather basic contact attributes like name, email, phone, and
address. All of those contact attributes are subject to change, and
sometimes frequent change. This makes matching contact data back up
into a consolidated list of individual persons&amp;nbsp;difficult.&lt;/p&gt;
&lt;p&gt;The problem of identifying which contacts in our data our duplicates
of each other is essentially a problem of identity&amp;nbsp;resolution.&lt;/p&gt;
&lt;p&gt;Luckily, because this is a common business problem, there are many
resources available to help address the issue. In particular, the free
and open source packages splink and zingg appeared promising to our
team. Both packages train and deploy machine learning models developed
from academic statistics research into identity&amp;nbsp;resolution.&lt;/p&gt;
&lt;p&gt;Machine learning models can make powerful, accurate predictions, but
may be non-deterministic and can obscure the logic used under-the-hood
to make their predictions. Without some kind of baseline, it would be
hard for us to tell how well these models were performing in
identifying duplicates in our data. To improve our understanding of
our problem, we first developed a series of &lt;span class="caps"&gt;SQL&lt;/span&gt; models to identify
duplicates according to a set of reliable, deterministic&amp;nbsp;criteria.&lt;/p&gt;
&lt;h4&gt;&lt;span class="caps"&gt;SQL&lt;/span&gt;&amp;nbsp;Models&lt;/h4&gt;
&lt;p&gt;We made two sets of criteria for identifying duplicates at different
levels of confidence. &amp;#8220;High confidence matches&amp;#8221; were matches that we
felt so confident of, we were willing to merge without manual
review. &amp;#8220;Medium confidence matches&amp;#8221; are mostly duplicates, but contain
many false positives that we want to weed out with either manual
review or future revisions of our &lt;span class="caps"&gt;SQL&lt;/span&gt;&amp;nbsp;models.&lt;/p&gt;
&lt;h5&gt;High confidence&amp;nbsp;matches&lt;/h5&gt;
&lt;p&gt;Our high confidence criteria are based on the same criteria that
EveryAction uses. By design there should be few or zero duplicates on
the contact sets that EveryAction uses to make matches, but we found
about 3% of our contacts were duplicates on these canonical contact
attributes. (Our support contact at EveryAction thinks these are
likely due to ongoing or legacy bugs in the matching&amp;nbsp;system).&lt;/p&gt;
&lt;p&gt;We expand this set by cleaning and normalizing the contact attributes
before they are&amp;nbsp;matched.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Names are cleaned of all non-alphabet or space characters,
  trimmed of leading or trailing&amp;nbsp;spaces&lt;/li&gt;
&lt;li&gt;All characters are&amp;nbsp;lower-cased&lt;/li&gt;
&lt;li&gt;First names are reduced to the first word before a&amp;nbsp;space&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This expanded set includes an additional ~0.3% of our total&amp;nbsp;contacts.&lt;/p&gt;
&lt;h5&gt;Medium confidence&amp;nbsp;matches&lt;/h5&gt;
&lt;p&gt;Our medium confidence criteria are a bit more expansive, and represent
different heuristics we developed based on looking through our data to
try and guess at what patterns were generating&amp;nbsp;duplicates. &lt;/p&gt;
&lt;p&gt;One very common pattern we noticed was contacts recorded with a first
initial as a first name. For example, &amp;#8220;Barbara Smith, 555-555-5555&amp;#8221;
may have a duplicate &amp;#8220;B Smith,&amp;nbsp;555-555-5555&amp;#8221;.&lt;/p&gt;
&lt;p&gt;Another common pattern we saw was a match in email usernames with
either a change or typo in email
domain. e.g. &amp;#8220;barbsmith1972@yahoo.com&amp;#8221; should match
&amp;#8220;barbsmith1972@gmail.com&amp;#8221; or&amp;nbsp;&amp;#8220;barbsmith1972@yahoo.co&amp;#8221;.&lt;/p&gt;
&lt;p&gt;This medium confidence model mimics the high confidence model, but
expands the match criteria to catch these first initial matches and
email username&amp;nbsp;matches.&lt;/p&gt;
&lt;p&gt;The other common pattern we catch in this model is matches on first
name and last name when these other contact attributes are all
null. As discussed above, when contact data with only first name and
last name are loaded to Every Action, a new contact is created every
time. We want to find and merge all such&amp;nbsp;examples.&lt;/p&gt;
&lt;p&gt;When we filter out overlaps with the high confidence model, this
medium confidence model finds an additional ~1% of our total&amp;nbsp;contacts.&lt;/p&gt;
&lt;p&gt;We do expect that some of these are false positives, and we want to
manually review all of these medium confidence matches before
programmatically merging&amp;nbsp;them.&lt;/p&gt;
&lt;h5&gt;Review&lt;/h5&gt;
&lt;p&gt;Using these clear and deterministic criteria, we find that 3-4% of our
total contacts may be duplicates. Likely some portion of these are
false positives, but we also expect that we capture only a small
portion of the total duplicates that fail to match on these particular&amp;nbsp;criteria.&lt;/p&gt;
&lt;p&gt;The contacts identified by our &lt;span class="caps"&gt;SQL&lt;/span&gt; models give us a useful baseline
for comparison to our machine learning models. We can now estimate
their performance by comparing the overlap in contacts identified by&amp;nbsp;each.&lt;/p&gt;
&lt;h4&gt;Machine Learning&amp;nbsp;Models&lt;/h4&gt;
&lt;p&gt;While deterministic criteria may work for identifying many potential
duplicates, there are likely many more whose similarity is less easily
defined. Typos or changes across multiple fields may make comparison
difficult, and even with fuzzy matching we would still want to
differentially weight matches on different attributes. For example, an
exact match on date of birth is worth more than an exact match on&amp;nbsp;zipcode.&lt;/p&gt;
&lt;p&gt;The free and open source packages &lt;a href="https://moj-analytical-services.github.io/splink/"&gt;splink&lt;/a&gt; and &lt;a href="https://www.zingg.ai/"&gt;zingg&lt;/a&gt; were both
designed by data scientists with way more expertise than myself in
this problem area, and I&amp;#8217;m inclined to trust that expertise and
implement their solutions to our identity resolution&amp;nbsp;problem.&lt;/p&gt;
&lt;p&gt;Both packages will take a dataset and output matches with confidence
scores. However, especially at first, I need to understand the
threshold at which the confidence of splink or zingg compares to my
own confidence. My initial approach is to send all matches above a
certain lower bound on confidence for manual review before
programmatically merging any of them. So I am treating all these
matches as &amp;#8220;medium&amp;nbsp;confidence&amp;#8221;.&lt;/p&gt;
&lt;p&gt;zingg looks a bit shinier, but splink looks easier to implement. zingg
and splink can both run on Apache Spark, but splink can also run on
duckdb. splink&amp;#8217;s duckdb backend was impressively capable of running
predictions on our entire contacts table on my &lt;span class="caps"&gt;16GB&lt;/span&gt; &lt;span class="caps"&gt;RAM&lt;/span&gt; laptop in a
few&amp;nbsp;minutes. &lt;/p&gt;
&lt;p&gt;I set up an automated workflow for splink to run as a Prefect flow and
load its predictions to a redshift table. The splink model needs some
configuration to tell it which records to compare and how different
attributes should be compared. I based my configuration off their official
tutorial which some minor changes to reflect what we know about
EveryAction contact&amp;nbsp;matches. &lt;/p&gt;
&lt;p&gt;splink makes a &lt;span class="caps"&gt;LOT&lt;/span&gt; of predicted matches. We set our lower confidence
bound around 0.3 (on a scale from 0-1) and about 20% of the contacts
in our database are predicted to be duplicates at this level of
confidence. We expect many of these to be false positive matches, and
can select some subset of the top matches to manually&amp;nbsp;validate.&lt;/p&gt;
&lt;p&gt;|    | sql confidence   |   count |   splink max |   splink min |   splink mean |   splink median |\n|&amp;#8212;-:|:&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;-|&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;:|&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;-:|&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;-:|&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;:|&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;&amp;#8212;:|\n|  0 | high             |   16562 |            1 |     0        |      0.997379 |        1        |\n|  1 | medium           |    8935 |            1 |     0        |      0.949009 |        0.958971 |\n|  2 | 0                | 6933193 |            1 |     0.300087 |      0.9452   |        0.944315&amp;nbsp;|&lt;/p&gt;
&lt;p&gt;This table compares how splink matches line up against our &lt;span class="caps"&gt;SQL&lt;/span&gt;
models. You can make your own conclusions, but I have a few key&amp;nbsp;takeaways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;splink makes several orders of magnitude more match predictions than
    my &lt;span class="caps"&gt;SQL&lt;/span&gt;&amp;nbsp;models&lt;/li&gt;
&lt;li&gt;splink confidence levels trend quite high, with a median of 94%
    confidence across all ~7 million predicted matches. Likely there
    is a huge difference in the quality of matches between 99%
    confidence and 94%&amp;nbsp;confidence&lt;/li&gt;
&lt;li&gt;splink confidence does seem to track with my &lt;span class="caps"&gt;SQL&lt;/span&gt; models. My high
    confidence &lt;span class="caps"&gt;SQL&lt;/span&gt; matches correspond to a median splink confidence of
    100%, down to 95.9% for my medium confidence &lt;span class="caps"&gt;SQL&lt;/span&gt; matches, and then
    down again to 94.4% for matches not identified by my &lt;span class="caps"&gt;SQL&lt;/span&gt;&amp;nbsp;models.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some manual review of splink&amp;#8217;s top matches that aren&amp;#8217;t covered by my
&lt;span class="caps"&gt;SQL&lt;/span&gt; models validates the expected behavior. splink makes matches
between contacts that are clearly duplicated on manual review, but
don&amp;#8217;t fall into the deterministic criteria I set in my &lt;span class="caps"&gt;SQL&lt;/span&gt;
models. Some of these&amp;nbsp;scenarios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A nickname is used in one contact, making a match on firstname&amp;nbsp;fail&lt;/li&gt;
&lt;li&gt;A small typo exists in an email address or street&amp;nbsp;address&lt;/li&gt;
&lt;li&gt;A contact&amp;#8217;s last name changed, perhaps due to&amp;nbsp;marriage&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;splink makes more match predictions than we can easily handle, and
seems to be performing well enough that we don&amp;#8217;t need to bother
setting up&amp;nbsp;zingg.&lt;/p&gt;
&lt;h3&gt;Manual&amp;nbsp;Validation&lt;/h3&gt;
&lt;p&gt;Merging contacts is an irreversible operation, so it is important to
be quite confident about matches before executing a merge. For matches
identified by the high confidence &lt;span class="caps"&gt;SQL&lt;/span&gt; model defined above, we feel
confident about a programmatic merge. However, for all the matches
identified by our &amp;#8220;medium confidence&amp;#8221; &lt;span class="caps"&gt;SQL&lt;/span&gt; model, and all matches
identified by our splink implementation, we want to execute a manual
validation before&amp;nbsp;merging. &lt;/p&gt;
&lt;p&gt;To manually validate contacts, we have set up a web application that
pulls matches and all associated contact details. Contacts and their
details are displayed side by side, and a user selects whether the
match is valid, invalid, or unclear. The validation workflow is
somewhat boring and tedious, but quite fast. Using the web app, a
single person can validate 500-1000 matches in an&amp;nbsp;hour. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Screenshot of validation app" src="https://austinweisgrau.github.io/images/validation_app.png"&gt;&lt;/p&gt;
&lt;p&gt;Over time, using the web app to manually validate contacts helps to
generate new &amp;#8220;high confidence&amp;#8221; heuristics that can be used to identify
matches without needing a manual review step. For example, we may
realize we have a large number of contacts with first name &amp;#8220;undefined&amp;#8221;
and last name &amp;#8220;undefined&amp;#8221;, and we can investigate those further and
purge them from the database. Another example is that we have found
that contacts that are matched on first name and last name but have no 
additional contact details defined can be merged with high&amp;nbsp;confidence.&lt;/p&gt;
&lt;h2&gt;Whereunto&amp;nbsp;??&lt;/h2&gt;
&lt;p&gt;EveryAction should take responsibility for the management of duplicate
contacts as a critical feature in their &lt;span class="caps"&gt;CRM&lt;/span&gt;. However, in lieu of them
fixing this problem, it is possible for users with sufficient
engineering capacity to understand the problem and implement a fix on
our&amp;nbsp;own. &lt;/p&gt;
&lt;p&gt;We use &lt;span class="caps"&gt;SQL&lt;/span&gt; queries to identify matches at two levels of confidence,
and we use splink to identify another batch of duplicates. We feel
confident we can programmatically merge higher confidence &lt;span class="caps"&gt;SQL&lt;/span&gt; matches
without review, but we want to manually validate all the other
matches. We can use a web application to make the manual validation a
quick and easy process and manually validate thousands of potential
matches in just a few&amp;nbsp;days. &lt;/p&gt;
&lt;p&gt;High confidence and validated matches are accumulated in a database
table and staged for processing by a programmatic merge script. This
script iterates through each match and executes an &lt;span class="caps"&gt;API&lt;/span&gt; call to the
EveryAction &lt;span class="caps"&gt;API&lt;/span&gt; endpoint &lt;code&gt;/people/{vanid}/mergeInto&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;We run our &lt;span class="caps"&gt;SQL&lt;/span&gt; models, splink, and the programmatic merge on a weekly
basis to identify and merge duplicates. Manual validation happens on a
rolling basis and all validated contacts are aggregated and merged on
the next schedule merge&amp;nbsp;flow.&lt;/p&gt;
&lt;p&gt;If you want to know more about any of this, feel free to reach out. I
also tentatively plan on making an affordable plug-and-play version of
this application available to &lt;span class="caps"&gt;TMC&lt;/span&gt; members in early 2025 - let me know
if you might be interested in beta testing&amp;nbsp;this.&lt;/p&gt;</content><category term="sharing"></category></entry><entry><title>Migrating from Civis to Prefect</title><link href="https://austinweisgrau.github.io/migrating-from-civis-to-prefect.html" rel="alternate"></link><published>2023-04-17T00:00:00-07:00</published><updated>2023-04-17T00:00:00-07:00</updated><author><name>Austin Weisgrau</name></author><id>tag:austinweisgrau.github.io,2023-04-17:/migrating-from-civis-to-prefect.html</id><summary type="html">&lt;h2&gt;The Problem: Data Orchestration in&amp;nbsp;Civis&lt;/h2&gt;
&lt;p&gt;Like many progressive political organizations, Working Families Party
has worked with Civis to set up and access our data
infrastructure. Civis is a user-friendly orchestration platform and
web application for executing &lt;span class="caps"&gt;SQL&lt;/span&gt; and python that interacts with our
redshift data&amp;nbsp;warehouse.&lt;/p&gt;
&lt;p&gt;Civis is an …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;The Problem: Data Orchestration in&amp;nbsp;Civis&lt;/h2&gt;
&lt;p&gt;Like many progressive political organizations, Working Families Party
has worked with Civis to set up and access our data
infrastructure. Civis is a user-friendly orchestration platform and
web application for executing &lt;span class="caps"&gt;SQL&lt;/span&gt; and python that interacts with our
redshift data&amp;nbsp;warehouse.&lt;/p&gt;
&lt;p&gt;Civis is an excellent tool for many progressive campaigns and
organizations that have limited engineering resources. Civis provides a
simple and accessible platform for analysts to easily interact with
data without worrying about data engineering concerns. However, for
more mature and organizations that operate beyond a single political
cycle with sophisticated data needs, Civis has many limitations that
make it &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-1-civis-woes.html"&gt;an inappropriate tool&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Civis trades off many important data engineering best-practices in
favor of ease-of-use. Civis&amp;#8217; design leads users to make minimal use of
version control, offers users limited visibility over production
resources, and is mostly incompatible with the Infrastructure-As-Code&amp;nbsp;paradigm.&lt;/p&gt;
&lt;p&gt;As a large organization with a long term political vision, we
determined that Working Families Party required a much more mature
toolset for its data&amp;nbsp;stack.&lt;/p&gt;
&lt;h2&gt;Prefect to the&amp;nbsp;Rescue&lt;/h2&gt;
&lt;p&gt;There are a lot of orchestration tools out there, and we wanted to
choose the best tool for the job. I had prior experience with Apache
Airflow, which is one of the most popular and mature data
orchestration tools on the market. Several colleagues in the
progressive data space had been eyeing alternatives to Civis for the
same reasons we were, and suggested Prefect as an attractive new tool
worth&amp;nbsp;evaluating. &lt;/p&gt;
&lt;p&gt;Prefect offers most of the features of Airflow, but was more compelling
to us for a few reasons. Airflow can be complicated to set up, and
managed deployments like Astronomer or &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;MWAA&lt;/span&gt; can end up being quite
expensive. Prefect offers a lower infrastructural lift for getting&amp;nbsp;started.&lt;/p&gt;
&lt;p&gt;Another advantage of Prefect over Airflow is that migrating existing
Python code to Prefect is much simpler than migrating to
Airflow. Airflow has an idiosyncratic implementation in Python, but
existing Python code can be turned into a working Prefect flow by
simply wrapping it in a&amp;nbsp;decorator.&lt;/p&gt;
&lt;p&gt;Overall, what Prefect offered us was the opportunity to implement many
data-engineering best practices that were difficult or impossible to
achieve in Civis. With Prefect, all of our code, containers,
orchestration, and deployments are configured together as code in a
single version-controlled&amp;nbsp;repository.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-2-prefect-aws-ecs.html"&gt;Our execution infrastructure&lt;/a&gt; is configured declaratively using an &lt;span class="caps"&gt;AWS&lt;/span&gt;
CodeFormation script to set up an execution layer in &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;ECS&lt;/span&gt;. We use
Github Actions to automate our continuous integration &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt;
deployment. Changes to Prefect flows, orchestration, and other parts
of our configuration are seamlessly deployed to Prefect Cloud &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; &lt;span class="caps"&gt;ECS&lt;/span&gt; on
pushes to&amp;nbsp;main.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-4-moving-a-script-from-civis-to-prefect.html"&gt;Migrating scripts from Civis to Prefect&lt;/a&gt; involves a trivial amount of
refactoring, but also offers opportunities to use Prefect features for
&lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-3-rate-limiting-api-calls.html"&gt;concurrent execution&lt;/a&gt;, integrated logging, secret management, automated
orchestration, and&amp;nbsp;testing.&lt;/p&gt;
&lt;p&gt;Now that we have worked in Prefect for several months, we&amp;#8217;ve seen many
improvements to our development cycle and workflow. We have more
complete visibility over our production resources, which is really
important as we work to understand &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; refactor a significant amount of
legacy&amp;nbsp;code.&lt;/p&gt;
&lt;p&gt;With everything version-controlled, all changes are easy to coherently
organize using version control best practices, which enables our whole
team to better understand, track, and debug ongoing changes to our&amp;nbsp;codebase. &lt;/p&gt;
&lt;p&gt;Version-controlled and automatically deployed orchestration
configuration is also a welcome improvement for us. When business
logic requires changes to the automations controlling when and how
scripts are triggered to run, we can make those changes in our
repository, push them, and trust that those changes will be reflected
in production without any further&amp;nbsp;work.&lt;/p&gt;
&lt;h2&gt;Wrapping&amp;nbsp;Up&lt;/h2&gt;
&lt;p&gt;In conclusion, migrating from Civis to Prefect has been a major
upgrade for the data team at Working Families Party. While Civis
served us well in the past, its limitations became evident as our
organization grew and our data needs became more sophisticated. Prefect
offered us a mature and comprehensive toolset for data orchestration,
enabling us to implement best practices and streamline our development&amp;nbsp;cycle.&lt;/p&gt;
&lt;p&gt;Overall, Prefect has provided us with a powerful and flexible solution
for data orchestration, empowering our organization to meet the
demands of our long-term political vision. By embracing Prefect, we
have set ourselves up for success in managing our data infrastructure
and driving our progressive campaigns&amp;nbsp;forward. &lt;/p&gt;</content><category term="sharing"></category></entry><entry><title>Migrating to Prefect, Part 4: Moving a Script from Civis to Prefect</title><link href="https://austinweisgrau.github.io/migrating-to-prefect-part-4-moving-a-script-from-civis-to-prefect.html" rel="alternate"></link><published>2023-03-27T00:00:00-07:00</published><updated>2023-03-27T00:00:00-07:00</updated><author><name>Austin Weisgrau</name></author><id>tag:austinweisgrau.github.io,2023-03-27:/migrating-to-prefect-part-4-moving-a-script-from-civis-to-prefect.html</id><summary type="html">&lt;p&gt;&lt;em&gt;This post is the fourth in a series about migrating off of Civis and
onto Prefect as our orchestration tool. The &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-1-civis-woes.html"&gt;first post&lt;/a&gt; is about
the limitations of Civis, and the &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-2-prefect-aws-ecs.html"&gt;second&lt;/a&gt; and &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-3-rate-limiting-api-calls.html"&gt;third&lt;/a&gt; posts are about
setting up and using&amp;nbsp;Prefect.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Once Prefect was adequately set up, we were …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This post is the fourth in a series about migrating off of Civis and
onto Prefect as our orchestration tool. The &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-1-civis-woes.html"&gt;first post&lt;/a&gt; is about
the limitations of Civis, and the &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-2-prefect-aws-ecs.html"&gt;second&lt;/a&gt; and &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-3-rate-limiting-api-calls.html"&gt;third&lt;/a&gt; posts are about
setting up and using&amp;nbsp;Prefect.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Once Prefect was adequately set up, we were ready to start moving our
scripts out of Civis and into Prefect. This process involves the
following&amp;nbsp;steps:&lt;/p&gt;
&lt;p&gt;Basic necessary&amp;nbsp;steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Copy the code and tests into a module in the Prefect&amp;nbsp;repository&lt;/li&gt;
&lt;li&gt;Add task and flow decorators, potentially rearranging and
   encapsulating the&amp;nbsp;code&lt;/li&gt;
&lt;li&gt;Swap out loggers and credential fetching&amp;nbsp;methods&lt;/li&gt;
&lt;li&gt;Add entry to scheduling&amp;nbsp;module&lt;/li&gt;
&lt;li&gt;Run locally and ensure everything&amp;nbsp;works&lt;/li&gt;
&lt;li&gt;Deploy and run in production and ensure everything&amp;nbsp;works&lt;/li&gt;
&lt;li&gt;Turn off civis automation, archive script in&amp;nbsp;Civis&lt;/li&gt;
&lt;li&gt;Update relevant team&amp;nbsp;documentation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bonus&amp;nbsp;steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Refactor control flow to take advantage of prefect orchestration
   (concurrency, retries,&amp;nbsp;etc)&lt;/li&gt;
&lt;li&gt;Refactor, expand tests and&amp;nbsp;documentation&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Example Civis&amp;nbsp;Script&lt;/h2&gt;
&lt;p&gt;We can use the following Civis script as an example to work&amp;nbsp;with.&lt;/p&gt;
&lt;p&gt;This script pulls a list of &lt;span class="caps"&gt;VAN&lt;/span&gt; IDs from a Redshift table and updates
these &lt;span class="caps"&gt;VAN&lt;/span&gt; IDs in EveryAction with an origin source&amp;nbsp;code. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;parsons&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Redshift&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;VAN&lt;/span&gt;

&lt;span class="n"&gt;van_source_code&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;12345&lt;/span&gt;

&lt;span class="n"&gt;van_client&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;VAN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;EveryAction&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;api_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;VAN_API_KEY_PASSWORD&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;query_results&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Table&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Redshift&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;select vanid from some_important_table&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;query_results&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;van_client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_person_code&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;vanid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;van_source_code&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Finished applying source codes.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Copy the code, add task and flow decorators,&amp;nbsp;encapsulate&lt;/h2&gt;
&lt;p&gt;Prefect is intended to be simple to implement with existing code. The
minimum change necessary to make a Prefect flow out of existing code
is to encapsulate the code within a method that has the
&lt;code&gt;@flow()&lt;/code&gt; decorator&amp;nbsp;applied.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;parsons&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Redshift&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;VAN&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;prefect&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;flow&lt;/span&gt;


&lt;span class="n"&gt;van_source_code&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;12345&lt;/span&gt;

&lt;span class="nd"&gt;@flow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Update VAN Source Codes Sync&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;update_van_source_codes&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;van_client&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;VAN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;EveryAction&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;api_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;VAN_API_KEY_PASSWORD&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;query_results&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Table&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Redshift&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;select vanid from some_important_table&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;query_results&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;van_client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_person_code&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;vanid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;van_source_code&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Finished applying source codes.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;update_van_source_codes&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Better yet is to break out some of the methods within the flow as
prefect tasks, to gain greater oversight and control over the&amp;nbsp;flow.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;parsons&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Redshift&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;VAN&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;prefect&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;flow&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;task&lt;/span&gt;


&lt;span class="n"&gt;van_source_code&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;12345&lt;/span&gt;

&lt;span class="nd"&gt;@task&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;fetch_query_results&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Table&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;query_results&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Redshift&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;select vanid from some_important_table&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;query_results&lt;/span&gt;

&lt;span class="nd"&gt;@task&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;apply_origin_source_code&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;van_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;source_code&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;van_client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_person_code&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;van_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;source_code&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@flow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Update VAN Source Codes Sync&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;update_van_source_codes&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;van_client&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;VAN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;EveryAction&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;api_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;VAN_API_KEY_PASSWORD&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;query_results&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;fetch_query_results&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;query_results&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;apply_origin_source_code&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;vanid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;van_source_code&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Finished applying source codes.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Swap out&amp;nbsp;loggers&lt;/h2&gt;
&lt;p&gt;Prefect is capable of capturing print statements as logs, but this
behavior is not enabled by default. If your code uses print
statements, you can define your flow&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;...&lt;/span&gt;

&lt;span class="nv"&gt;@flow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Update VAN Source Codes Sync&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;log_prints&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;update_van_source_codes&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can set print logging globally with the Prefect config variable
&lt;code&gt;PREFECT_LOGGING_LOG_PRINTS=True&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can also alternatively use the prefect&amp;nbsp;logger.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;prefect&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;flow&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;task&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;get_run_logger&lt;/span&gt;

&lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="nd"&gt;@flow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Update VAN Source Codes Sync&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;update_van_source_codes&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;get_run_logger&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Finished apply source codes.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Swap out credential fetching&amp;nbsp;methods&lt;/h2&gt;
&lt;p&gt;Now that we&amp;#8217;re out of Prefect, we&amp;#8217;re not bound to using environment
variables with those pesky mandatory &lt;code&gt;_PASSWORD&lt;/code&gt; suffixes!&lt;/p&gt;
&lt;p&gt;You can fetch credentials using any implementation that makes sense
for your team and Prefect stack. You can add environment variables
to your Docker image, or fetch them from a secret store like &lt;span class="caps"&gt;AWS&lt;/span&gt;
Secrets Manager or use Prefect Blocks to store&amp;nbsp;credentials. &lt;/p&gt;
&lt;h2&gt;Add entry to scheduling&amp;nbsp;module&lt;/h2&gt;
&lt;p&gt;In my prefect template, automated flow scheduling is controlled by &lt;a href="https://github.com/austinweisgrau/prefect-ecs-template/blob/main/scheduling.py"&gt;a
scheduling module&lt;/a&gt; that is run by a &lt;a href="https://github.com/austinweisgrau/prefect-ecs-template/blob/b03c2db49209ae4f76f9a1c73db39d4bf0d8634d/.github/workflows/main.yaml#L151"&gt;github actions script&lt;/a&gt; if changes are
pushed to the main&amp;nbsp;branch.&lt;/p&gt;
&lt;p&gt;When a new flow is created, a block needs to be added to the
scheduling module to set up automated runs for this&amp;nbsp;flow.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;update_deployment_schedules&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;None&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;schedule_deployment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;update_van_source_codes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Update VAN Source Codes Sync&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timedelta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Daily&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;minute&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;second&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;At&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;05&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pm&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Run locally and ensure everything&amp;nbsp;works&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$&lt;span class="w"&gt; &lt;/span&gt;python&lt;span class="w"&gt; &lt;/span&gt;flows/update_van_source_codes/update_van_source_codes_flow.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Deploy&lt;/h2&gt;
&lt;p&gt;If you are using a continuous deployment script, you can push the new
code to your main branch and let that script create the new deployment
in the Prefect Cloud. You can also create the deployment manually by
running a command&amp;nbsp;like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$&lt;span class="w"&gt; &lt;/span&gt;prefect&lt;span class="w"&gt; &lt;/span&gt;deployment&lt;span class="w"&gt; &lt;/span&gt;build&lt;span class="w"&gt; &lt;/span&gt;flows/update_van_source_codes/update_van_source_codes_flow.py:update_van_source_codes&lt;span class="w"&gt; &lt;/span&gt;-a&lt;span class="w"&gt; &lt;/span&gt;-n&lt;span class="w"&gt; &lt;/span&gt;update_van_source_codes&lt;span class="w"&gt; &lt;/span&gt;-ib&lt;span class="w"&gt; &lt;/span&gt;ecs-task/prod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Refactor control flow to take advantage of prefect&amp;nbsp;orchestration&lt;/h2&gt;
&lt;p&gt;Instead of the for loop for executing our &lt;span class="caps"&gt;API&lt;/span&gt; calls, we can use
task.submit() or task.map() to take advantage of prefect&amp;nbsp;concurrency.&lt;/p&gt;
&lt;p&gt;Using our custom &lt;a href=""&gt;task concurrency limiter&lt;/a&gt;, we can set a rate limit of
3 as advised by the &lt;a href="https://docs.ngpvan.com/docs/throttling-guidelines"&gt;EveryAction documentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;utilities.concurrency&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;limit_concurrency&lt;/span&gt;

&lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="nd"&gt;@task&lt;/span&gt;
&lt;span class="nd"&gt;@limit_concurrency&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;apply_origin_source_code&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;van_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;source_code&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;van_client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_person_code&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;van_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;source_code&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="nd"&gt;@flow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Update VAN Source Codes Sync&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;update_van_source_codes&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;van_client&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;VAN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;EveryAction&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;api_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;VAN_API_KEY_PASSWORD&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;query_results&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;fetch_query_results&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;van_ids&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;vanid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;query_results&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;# Await results before finishing flow&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;apply_origin_source_code&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;van_ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;van_source_code&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Finished applying source codes.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Refactor, expand tests and&amp;nbsp;documentation&lt;/h2&gt;
&lt;p&gt;Every time you are looking anew at a script is a great opportunity to
clean up the code, notice where it is unclear, and documentation to
clarify technical logic and business logic, and add tests to validate
important pieces of the&amp;nbsp;code!&lt;/p&gt;</content><category term="sharing"></category></entry><entry><title>Migrating to Prefect, Part 2: Prefect &amp; AWS ECS</title><link href="https://austinweisgrau.github.io/migrating-to-prefect-part-2-prefect-aws-ecs.html" rel="alternate"></link><published>2023-03-07T00:00:00-08:00</published><updated>2023-03-07T00:00:00-08:00</updated><author><name>Austin Weisgrau</name></author><id>tag:austinweisgrau.github.io,2023-03-07:/migrating-to-prefect-part-2-prefect-aws-ecs.html</id><summary type="html">&lt;p&gt;&lt;em&gt;In my &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-1-civis-woes.html"&gt;previous blog post&lt;/a&gt;, I explained the limitations with Civis
as a production tool for a data/engineering team. These limitations
led our data team at Working Families Party to choose to move our
orchestration tooling off of Civis and into another tool. After
evaluating a few tools, we …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;In my &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-1-civis-woes.html"&gt;previous blog post&lt;/a&gt;, I explained the limitations with Civis
as a production tool for a data/engineering team. These limitations
led our data team at Working Families Party to choose to move our
orchestration tooling off of Civis and into another tool. After
evaluating a few tools, we were excited to begin our migration to&amp;nbsp;Prefect.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This post will explain how we set up Prefect and some of the choices
we made along the way. You can follow along with the full set up as a
template repository &lt;a href="https://github.com/austinweisgrau/prefect-ecs-template"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, just one vocabulary word that will make the rest of this post
more&amp;nbsp;readable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prefect &amp;#8220;Flow&amp;#8221;: this is more generically called a script or data
  pipeline. Prefect is fundamentally about the orchestration,
  execution, and observability of&amp;nbsp;flows.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Orchestration and Execution in&amp;nbsp;Prefect&lt;/h2&gt;
&lt;p&gt;Orchestration tools handle the separation of orchestration and
execution differently. Some tools, like Civis and Airflow, handle both
of these in the same layer. The same platform that schedules and
triggers workflows is also responsible for executing them, often in
the same compute&amp;nbsp;environment.&lt;/p&gt;
&lt;p&gt;Prefect handles this differently and fully separates the
orchestration and compute layers. The orchestration layer is run in a
cloud server instance running an &lt;span class="caps"&gt;API&lt;/span&gt;-first web application with a &lt;span class="caps"&gt;UI&lt;/span&gt; for
viewing and interacting with Prefect assets. Prefect offers a managed,
hosted version of the server called the Prefect Cloud, with a
generous free tier. (More documentation about Prefect&amp;#8217;s design &lt;a href="https://www.prefect.io/why-prefect/hybrid-model/"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The Prefect execution layer happens separately, and there is currently
no managed option (although one is slated for release in the coming
months). A Prefect &amp;#8220;agent&amp;#8221; or &amp;#8220;worker&amp;#8221; is a program that runs in
the execution environment, frequently polls the Prefect orchestration
server, and executes any flows that have been triggered by the
server. Prefect users are responsible for setting up the environment
for a Prefect agent or worker to&amp;nbsp;run.&lt;/p&gt;
&lt;h2&gt;Cloud execution&amp;nbsp;options&lt;/h2&gt;
&lt;p&gt;While it is possible to run Prefect flows locally, for example on the
laptops of your team&amp;#8217;s staff, a more robust solution is to run a
Prefect agent in a cloud computing environment. There are many
different ways to accomplish&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;A common method is to use cloud &amp;#8220;serverless&amp;#8221; compute infrastructure to
run the Prefect agent. Some popular options include &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;EC2&lt;/span&gt;, &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;ECS&lt;/span&gt;,
Azure Container Apps, or Google Cloud&amp;nbsp;Run.&lt;/p&gt;
&lt;p&gt;Some organizations with more resources might even choose to run their
execution layer in a cloud-hosted Kubernetes cluster. Kubernetes is
very configurable and scalable, but has a steeper and more complicated
learning&amp;nbsp;curve.&lt;/p&gt;
&lt;h2&gt;Tutorials and&amp;nbsp;templates&lt;/h2&gt;
&lt;p&gt;One of the limitations of using Prefect is that the tool is newer,
the community is smaller, and there are not as many resources
available yet (compared to more mature tools like Airflow)
demonstrating how to accomplish various set-ups. However, there are a
few excellent templates and tutorials for getting the Prefect execution
layer set up in various cloud&amp;nbsp;environments.&lt;/p&gt;
&lt;p&gt;We used the &amp;#8220;dataflow-ops&amp;#8221; &lt;a href="https://towardsdatascience.com/prefect-aws-ecs-fargate-github-actions-make-serverless-dataflows-as-easy-as-py-f6025335effc"&gt;tutorial&lt;/a&gt; and &lt;a href="https://github.com/anna-geller/dataflow-ops"&gt;template&lt;/a&gt; to get our set up
started in &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;ECS&lt;/span&gt;. We chose to use &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;ECS&lt;/span&gt; largely because we use &lt;span class="caps"&gt;AWS&lt;/span&gt;
Redshift and &lt;span class="caps"&gt;AWS&lt;/span&gt; S3 as our primary database and data storage layers,
and there are significant performance benefits to using &lt;span class="caps"&gt;AWS&lt;/span&gt; services&amp;nbsp;together.&lt;/p&gt;
&lt;p&gt;Alternatively, &lt;a href="https://medium.com/@nwosupaul141/serverless-deployment-of-a-prefect-data-pipeline-on-google-cloud-run-8c48765f2480"&gt;this tutorial&lt;/a&gt; looks useful for getting started in Google
Cloud Run. We may switch to this implementation if we end up moving
off of Redshift and into Google Biguery at some&amp;nbsp;point.&lt;/p&gt;
&lt;h2&gt;Needing a dedicated &lt;span class="caps"&gt;IP&lt;/span&gt; for access to&amp;nbsp;redshift&lt;/h2&gt;
&lt;p&gt;The Movement Cooperative (&lt;span class="caps"&gt;TMC&lt;/span&gt;) is a cooperatively run organization
that supports progressive organizations run their tech stacks. &lt;span class="caps"&gt;TMC&lt;/span&gt;
manages our organization&amp;#8217;s data warehouse in&amp;nbsp;Redshift.&lt;/p&gt;
&lt;p&gt;We needed to enable a dedicated &lt;span class="caps"&gt;IP&lt;/span&gt; address for our execution
layer so that we could have our &lt;span class="caps"&gt;IP&lt;/span&gt; address whitelisted by &lt;span class="caps"&gt;TMC&lt;/span&gt; in order
to access the Redshift instance. Setting up a dedicated &lt;span class="caps"&gt;IP&lt;/span&gt; address for
an &lt;span class="caps"&gt;ECS&lt;/span&gt; Task involved an understanding of some networking concepts and
tools in &lt;span class="caps"&gt;AWS&lt;/span&gt;. Some new vocab words for me: &amp;#8220;&lt;span class="caps"&gt;VPC&lt;/span&gt;&amp;#8221;, &amp;#8220;subnet&amp;#8221;, &amp;#8220;&lt;span class="caps"&gt;NAT&lt;/span&gt;&amp;nbsp;gateway&amp;#8221;.&lt;/p&gt;
&lt;p&gt;The dataflow-ops template included an &lt;a href="https://github.com/anna-geller/dataflow-ops/blob/main/infrastructure/ecs_cluster_prefect_agent.yml"&gt;&lt;span class="caps"&gt;AWS&lt;/span&gt; CloudFormation Script&lt;/a&gt; for
setting up the &lt;span class="caps"&gt;ECS&lt;/span&gt; Task and its networking infrastructure. I modified
that script using &lt;a href="https://gist.github.com/jbesw/f9401b4c52a7446ef1bb71ceea8cc3e8"&gt;this template&lt;/a&gt; to enable the use of an &amp;#8220;elastic &lt;span class="caps"&gt;IP&lt;/span&gt;&amp;#8221;
address in &lt;span class="caps"&gt;AWS&lt;/span&gt; for our &lt;span class="caps"&gt;ECS&lt;/span&gt; tasks. I also needed to modify the
&lt;code&gt;prefect_aws.ECSTask&lt;/code&gt; block stored in our Prefect Cloud instance to
always use the correct (private) &lt;span class="caps"&gt;VPC&lt;/span&gt; subnets (see our &lt;a href="https://github.com/austinweisgrau/prefect-ecs-template/blob/main/infrastructure/ecs_cluster_prefect_agent.yml"&gt;full configuration script for CloudFormation here&lt;/a&gt;). &lt;/p&gt;
&lt;h2&gt;Deploying&amp;nbsp;Changes&lt;/h2&gt;
&lt;p&gt;There are two different ways a Prefect flow can be deployed which
determine how the code is passed to the Prefect agent and&amp;nbsp;executed.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Prefect agent and/or ECSTask can run on a docker image that
   contains the flow&amp;nbsp;code&lt;/li&gt;
&lt;li&gt;The &lt;a href="https://docs.prefect.io/concepts/deployments/#deployment-build-options"&gt;flow deployment can be built&lt;/a&gt; with a &lt;a href="https://docs.prefect.io/concepts/deployments/#block-identifiers"&gt;cloud &amp;#8220;storage block&amp;#8221;&lt;/a&gt; (for
   example, S3), where the flow code will be uploaded on deployment
   and downloaded when the flow is triggered by the agent to&amp;nbsp;run.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When changes are made to flow code, the updates can be pushed into
production either by recreating and reuploading the docker image
(strategy 1), or by rebuilding the flow deployment using a storage
block (strategy 2), which uploads the new code to the storage&amp;nbsp;block.&lt;/p&gt;
&lt;p&gt;I found the docker strategy to be somewhat simpler, so I modified the
Github Actions deployment script included in the &lt;a href="https://github.com/anna-geller/dataflow-ops/blob/main/.github/workflows/main.yaml"&gt;dataflow-ops template&lt;/a&gt;
to rebuild and reupload the docker image anytime the code is
updated. Our flow deployments do not include a storage&amp;nbsp;block.&lt;/p&gt;
&lt;p&gt;See our &lt;a href="https://github.com/austinweisgrau/Prefect-ecs-template/blob/main/.github/workflows/main.yaml"&gt;deployment Github Actions script here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The next post in this series explores a &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-3-rate-limiting-api-calls.html"&gt;useful utility for working in Prefect&lt;/a&gt;: rate-limiting &lt;span class="caps"&gt;API&lt;/span&gt;&amp;nbsp;calls. &lt;/p&gt;</content><category term="sharing"></category></entry><entry><title>Migrating to Prefect, Part 3: Rate limiting API calls</title><link href="https://austinweisgrau.github.io/migrating-to-prefect-part-3-rate-limiting-api-calls.html" rel="alternate"></link><published>2023-03-02T00:00:00-08:00</published><updated>2023-03-02T00:00:00-08:00</updated><author><name>Austin Weisgrau</name></author><id>tag:austinweisgrau.github.io,2023-03-02:/migrating-to-prefect-part-3-rate-limiting-api-calls.html</id><summary type="html">&lt;p&gt;&lt;em&gt;This post is the third in a series about migrating off of Civis and
onto Prefect as our orchestration tool. The &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-1-civis-woes.html"&gt;first post&lt;/a&gt; is about the
limitations of Civis, and the &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-2-prefect-aws-ecs.html"&gt;second post&lt;/a&gt; is about setting up Prefect
with &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;ECS&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A common, basic workflow in a data pipeline is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This post is the third in a series about migrating off of Civis and
onto Prefect as our orchestration tool. The &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-1-civis-woes.html"&gt;first post&lt;/a&gt; is about the
limitations of Civis, and the &lt;a href="https://austinweisgrau.github.io/migrating-to-prefect-part-2-prefect-aws-ecs.html"&gt;second post&lt;/a&gt; is about setting up Prefect
with &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;ECS&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A common, basic workflow in a data pipeline is to make concurrent,
rate-limited &lt;span class="caps"&gt;API&lt;/span&gt; calls to fetch or post data to a service.
One obstacle I hit early on when attempting to move our data pipelines
into Prefect was around setting up rate-limited &lt;span class="caps"&gt;API&lt;/span&gt;&amp;nbsp;calls.&lt;/p&gt;
&lt;p&gt;&lt;a href="#solution"&gt;Skip to solution&lt;/a&gt; or see &lt;a href="https://github.com/austinweisgrau/prefect-ecs-template/blob/main/flows/api_call_demonstration/api_call_flow.py"&gt;a working example in my prefect template&amp;nbsp;repository&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Implementing rate-limited &lt;span class="caps"&gt;API&lt;/span&gt; calls in standard&amp;nbsp;python&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://realpython.com/python-concurrency/#when-is-concurrency-useful"&gt;Python has two paradigms for executing &lt;span class="caps"&gt;IO&lt;/span&gt;-bound concurrency&lt;/a&gt;: asyncio
and&amp;nbsp;multithreading.&lt;/p&gt;
&lt;p&gt;asyncio is a very powerful framework, but involves a somewhat
substantial change to python patterns. It cannot be easily added to
existing code without rewriting the entire codebase for compatibility
with asyncio. It also represents a technical hurdle and maintenance
burden on smaller teams. For a simple workflow like rate-limited &lt;span class="caps"&gt;API&lt;/span&gt;
calls, using asyncio would be significant&amp;nbsp;overkill.&lt;/p&gt;
&lt;p&gt;Multithreading can be complicated to set up, but the python standard
library includes a high-level &lt;span class="caps"&gt;API&lt;/span&gt; to simplify the use of
multithreading: &lt;code&gt;concurrency.futures&lt;/code&gt;. I like to use the
&lt;code&gt;ThreadPoolExecutor&lt;/code&gt;, which can be easily configured to limit the
number of active&amp;nbsp;threads. &lt;/p&gt;
&lt;p&gt;A standard implementation of concurrent rate-limited &lt;span class="caps"&gt;API&lt;/span&gt; calls in
python can look like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;concurrent.futures&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ThreadPoolExecutor&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_api_call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Response&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;api_url_endpoint&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;ThreadPoolExecutor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;payloads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="n"&gt;responses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;make_api_call&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;payloads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This code will execute an &lt;span class="caps"&gt;API&lt;/span&gt; call on each payload in the list of
payloads, with 3 threads running simultaneously, each executing one
&lt;span class="caps"&gt;API&lt;/span&gt; call at a time. If each call takes 1 full second, executing 30
calls will take about 10 seconds to&amp;nbsp;complete.&lt;/p&gt;
&lt;p&gt;This represents a speed increase of 3x over using a for loop, which
would take a full 30 seconds to&amp;nbsp;complete. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;responses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;payload&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;payloads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_api_call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;responses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Implementing concurrency in&amp;nbsp;Prefect&lt;/h2&gt;
&lt;p&gt;Prefect has several different configurable mechanisms for
orchestrating concurrent execution of python&amp;nbsp;code.&lt;/p&gt;
&lt;p&gt;Prefect is built on top of an asyncio implementation called AnyIO and
can seamlessly work with python scripts written using asyncio. For
reasons described above, I don&amp;#8217;t want to use&amp;nbsp;asyncio.&lt;/p&gt;
&lt;p&gt;Prefect tasks are the main building block of flows in Prefect, and are
also the main mechanism for orchestrating concurrency. Prefect tasks
called with &lt;code&gt;task.submit()&lt;/code&gt; or &lt;code&gt;task.map()&lt;/code&gt; are sent to a &lt;a href="https://docs.prefect.io/concepts/task-runners/"&gt;Task Runner&lt;/a&gt;
for (potentially) concurrent execution. Sequential, concurrent or parallel
execution will occur depending on which task runner is used. The
default task runner is a&amp;nbsp;ConcurrentTaskRunner.&lt;/p&gt;
&lt;p&gt;Tasks submitted to a concurrent runner will all execute as soon as
they are not waiting for any upstream&amp;nbsp;values.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;prefect&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;flow&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;task&lt;/span&gt;

&lt;span class="nd"&gt;@task&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_api_call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Response&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;api_url_endpoint&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;

&lt;span class="nd"&gt;@flow&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;my_flow&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;payloads&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="n"&gt;responses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_api_call&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;payloads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Using prefect makes concurrent execution of python code wonderfully&amp;nbsp;simple.&lt;/p&gt;
&lt;p&gt;In the above implementation, if each &lt;span class="caps"&gt;API&lt;/span&gt; call takes 1 second to
complete and there are 30 total &lt;span class="caps"&gt;API&lt;/span&gt; calls, the full flow will be done
in about 1 second, because all the calls execute&amp;nbsp;simultaneously.&lt;/p&gt;
&lt;h2&gt;Rate limiting concurrency in&amp;nbsp;Prefect&lt;/h2&gt;
&lt;p&gt;We still need to implement rate limiting on our &lt;span class="caps"&gt;API&lt;/span&gt; calls in
Prefect. The code above will execute every &lt;span class="caps"&gt;API&lt;/span&gt; call across all the
payloads simultaneously. This will generally get your &lt;span class="caps"&gt;API&lt;/span&gt; calls
temporarily or permanently&amp;nbsp;blocked.&lt;/p&gt;
&lt;p&gt;Prefect offers a native solution for limiting concurrency on tasks:
simply enough, a &lt;a href="https://docs.prefect.io/concepts/tasks/#task-run-concurrency-limits"&gt;task run concurrency limit&lt;/a&gt;. It is simple to set up and
use. Concurrency limit values must be deployed to the server&amp;nbsp;with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$&lt;span class="w"&gt; &lt;/span&gt;prefect&lt;span class="w"&gt; &lt;/span&gt;concurrency-limit&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;limit_concurrency_10&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And then prefect tasks can be rate limited by tagging them with the
name of the concurrency&amp;nbsp;limit:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nd"&gt;@task&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tag&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;limit_concurrency_10&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_api_call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
   &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, rather than limiting the number of tasks that can run at any
given moment, this mechanism limits the number of tasks that can run
in any given 30 second block. Prefect tasks with a rate limit check
for an open slot, and if none is available, wait 30 seconds to check
again for an open&amp;nbsp;slot. &lt;/p&gt;
&lt;p&gt;This is quite problematic, as it results in very significant slowdowns
of rate-limited execution over the standard python use of
multithreading. If you only want 3 calls at a time, you must set up a
concurrency limit of 3. However, after those 3 calls execute, no more
calls will begin to execute until that initial 30 second window has
closed. If each call takes 1 full second, running 30 calls will take
15 minutes to&amp;nbsp;complete!&lt;/p&gt;
&lt;p&gt;The Prefect team is aware of this issue and is working on an improved
implementation of the task-concurrency feature. See &lt;a href="https://github.com/PrefectHQ/prefect/issues/8873"&gt;this github issue&lt;/a&gt;,
&lt;a href="https://github.com/PrefectHQ/prefect/pull/7013"&gt;this github &lt;span class="caps"&gt;PR&lt;/span&gt;&lt;/a&gt;, and this &lt;a href="https://prefect-community.slack.com/archives/C03D12VV4NN/p1677533662427229"&gt;slack discussion&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Solution&lt;/h2&gt;
&lt;p&gt;One simple solution would be to use normal python mulithreading code
within a prefect task. In Prefect, however, a Prefect task is intended
to be the smallest unit of concurrency. It breaks intended Prefect
patterns to use multithreading from within a prefect task. Some Prefect
utilities are incompatible with multithreaded task code. &lt;a href="https://github.com/PrefectHQ/prefect/issues/8652"&gt;&lt;em&gt;See discussion&amp;nbsp;here.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the end, the approach I settled on with some help from the Prefect
development team involved using a python &lt;a href="https://superfastpython.com/thread-semaphore/"&gt;&lt;code&gt;threading.Semaphore&lt;/code&gt;&lt;/a&gt;. A
Semaphore is a standard approach for rate limiting multithreaded python
code, used behind the scenes by high-level APIs like
&lt;code&gt;concurrent.futures.ThreadPoolExecutor&lt;/code&gt;. Each thread attempts to access
an open &amp;#8220;slot&amp;#8221; from the Semaphore, and waits until a slot is available
to run. While it runs, it holds a slot, and releases that slot once it
is&amp;nbsp;complete.&lt;/p&gt;
&lt;p&gt;When Prefect tasks are submitted to a ConcurrentTaskRunner, they are
executed in the same python process using multithreading behind the
scenes, which means using a Semaphore is a natural&amp;nbsp;solution.&lt;/p&gt;
&lt;p&gt;My implementation of the prefect-compatible semaphore-based
concurrency rate limiting can be found below or in my template
repository &lt;a href="https://github.com/austinweisgrau/prefect-ecs-template/blob/main/utilities/concurrency.py"&gt;here&lt;/a&gt;, and its use ends up being very&amp;nbsp;simple:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;prefect&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;flow&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;task&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;utilities.concurrency&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;limit_concurrency&lt;/span&gt;

&lt;span class="nd"&gt;@task&lt;/span&gt;
&lt;span class="nd"&gt;@limit_concurrency&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_api_call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Response&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;api_url_endpoint&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;

&lt;span class="nd"&gt;@flow&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;my_flow&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;payloads&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="n"&gt;responses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_api_call&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;payloads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Identical to the implementation with &lt;code&gt;ThreadPoolExecutor&lt;/code&gt; above,
executing 30 &lt;span class="caps"&gt;API&lt;/span&gt; calls that take 1 second each will take about 10
seconds with this&amp;nbsp;implementation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="solution"&gt;&lt;/a&gt;My concurrency limiting task&amp;nbsp;decorator&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;wraps&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;threading&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Semaphore&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;typing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;limit_concurrency&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_workers&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Wraps methods to implement concurrency limit&lt;/span&gt;

&lt;span class="sd"&gt;    Prefect task concurrency limits use a 30 second delay between each&lt;/span&gt;
&lt;span class="sd"&gt;    check for an available slot. This is a more performative approach&lt;/span&gt;
&lt;span class="sd"&gt;    using a threading.Semaphore.&lt;/span&gt;

&lt;span class="sd"&gt;    Prefect must be using a &amp;quot;local&amp;quot; task runner for this to work (the&lt;/span&gt;
&lt;span class="sd"&gt;    ConcurrentTaskRunner) and not a distributed task runner like Dask&lt;/span&gt;
&lt;span class="sd"&gt;    or Ray.&lt;/span&gt;

&lt;span class="sd"&gt;    Usage:&lt;/span&gt;
&lt;span class="sd"&gt;      from prefect import task&lt;/span&gt;

&lt;span class="sd"&gt;      @task&lt;/span&gt;
&lt;span class="sd"&gt;      @limit_concurrency(max_workers=5)&lt;/span&gt;
&lt;span class="sd"&gt;      def my_task():&lt;/span&gt;
&lt;span class="sd"&gt;          pass&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="n"&gt;semaphore&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Semaphore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_workers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pseudo_decorator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nd"&gt;@wraps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;limited_concurrent_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;semaphore&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;limited_concurrent_func&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pseudo_decorator&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="sharing"></category></entry><entry><title>Migrating to Prefect. Part 1: Civis Woes</title><link href="https://austinweisgrau.github.io/migrating-to-prefect-part-1-civis-woes.html" rel="alternate"></link><published>2023-02-22T00:00:00-08:00</published><updated>2023-02-22T00:00:00-08:00</updated><author><name>Austin Weisgrau</name></author><id>tag:austinweisgrau.github.io,2023-02-22:/migrating-to-prefect-part-1-civis-woes.html</id><summary type="html">&lt;p&gt;&lt;em&gt;This is the first post in a series of blog posts about our migration
to Prefect at Working Families Party. I&amp;#8217;ll start by describing our
current orchestration and execution platform, Civis, and its many&amp;nbsp;limitations.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In a data context, &amp;#8220;orchestration&amp;#8221; refers to the work of coordinating,
scheduling, and automating …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This is the first post in a series of blog posts about our migration
to Prefect at Working Families Party. I&amp;#8217;ll start by describing our
current orchestration and execution platform, Civis, and its many&amp;nbsp;limitations.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In a data context, &amp;#8220;orchestration&amp;#8221; refers to the work of coordinating,
scheduling, and automating the movement and transformation of data
from its raw sources to everywhere it is needed in an
organization. This work is often conceptualized in terms of a series
of &amp;#8220;data pipelines&amp;#8221;, which extract data from sources, load it to
destinations, and transform it along the&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://civisanalytics.com"&gt;Civis&lt;/a&gt; is a widely used orchestration platform across the progressive
political space. Civis is an afforadable and accessible platform for
writing and running &lt;span class="caps"&gt;SQL&lt;/span&gt; and python in a cloud&amp;nbsp;environment.&lt;/p&gt;
&lt;p&gt;Civis is an all-inclusive data platform that allows campaigns and
organizations to manipulate data in their Redshift warehouse, automate
&lt;span class="caps"&gt;SQL&lt;/span&gt; and Python jobs, run scripts on containers, and more. They also
have an intuitive &lt;span class="caps"&gt;SQL&lt;/span&gt; to G Sheet export functionality that tend be the
bread and butter for a lot of political organizing. Civis is very user
friendly and does not require any engineering or &amp;#8220;devops&amp;#8221; experience
to get up and running. Civis has a lot going for it and there are good
reasons it is&amp;nbsp;popular.&lt;/p&gt;
&lt;p&gt;However, Civis is not an appropriate production tool for mature data
or engineering teams. Civis has many limitations, and many of its
design choices in favor of ease-of-use end up as liabilities in the
long term. For any team that is regularly running &lt;span class="caps"&gt;SQL&lt;/span&gt; and python
scripts with nontrivial logic, it is important to explore issues with
Civis as a production&amp;nbsp;tool. &lt;/p&gt;
&lt;h2&gt;Version&amp;nbsp;control&lt;/h2&gt;
&lt;p&gt;Version control is a critical tool for engineering at
every level. It enables collaboration, oversight, institutional
memory, an additional form of documentation, and more. There are tons
of resources online expounding the benefits and necessities of version&amp;nbsp;control.&lt;/p&gt;
&lt;p&gt;Using Civis as a production environment severely limits the extent to
which version controlling is possible. This problem alone is
significant enough to make Civis a bad choice, even if there were no
other&amp;nbsp;issues. &lt;/p&gt;
&lt;p&gt;Scripts that are stored in Civis are not version controlled. It is
possible to exclusively use Civis container scripts to run scripts
that are version controlled through Civis. But even in that scenario,
none of the configuration parameters, automated scheduling, workflow
dependencies, or credentials are version&amp;nbsp;controlled. &lt;/p&gt;
&lt;p&gt;Much more common is to have a large proliferation of production and
development scripts stored, shared and run exclusively in&amp;nbsp;Civis.&lt;/p&gt;
&lt;p&gt;As an example to demonstrate why this is problematic, imagine
that a team member accidentally deletes a single character from a &lt;span class="caps"&gt;SQL&lt;/span&gt;
script while they are reading the script one day, causing the output
of the &lt;span class="caps"&gt;SQL&lt;/span&gt; script to change slightly in a problematic way that isn&amp;#8217;t
obvious. Next week, another team notices that there are issues in a
database table that didn&amp;#8217;t previously exist. Your job is to determine
why this table isn&amp;#8217;t populating correctly, even though officially no
one has touched any of the relevant code in many&amp;nbsp;months.&lt;/p&gt;
&lt;p&gt;If the pipeline is complicated in any way, you will have to pull up
each individual script in Civis and check to see if the file was
updated recently. You find that this &lt;span class="caps"&gt;SQL&lt;/span&gt; script was updated last week,
but you can&amp;#8217;t see what the change was. You have to parse and test the
entire &lt;span class="caps"&gt;SQL&lt;/span&gt; script manually to determine why the results aren&amp;#8217;t correct,
and invent a new solution that hopefully restores the original intended&amp;nbsp;behavior. &lt;/p&gt;
&lt;p&gt;If version control was in place, your first step would be to check the
logs, and see that a small change was made last week that wasn&amp;#8217;t
intended, and to revert the change. More realistically, the change
would never have been made, because your team member would have had to
commit the change, issue a &lt;span class="caps"&gt;PR&lt;/span&gt;, have the &lt;span class="caps"&gt;PR&lt;/span&gt; approved, and merge the
change before it showed up in the production environment at&amp;nbsp;all.&lt;/p&gt;
&lt;p&gt;There are many, many other scenarios where version control is a
critical element of keeping a team functioning in the long&amp;nbsp;term.&lt;/p&gt;
&lt;h2&gt;Environment&amp;nbsp;variables&lt;/h2&gt;
&lt;p&gt;A Civis container can run a python script fetched from a github repo
on a Docker container of your choosing. A common pattern for fetching
credentials in python is to fetch them from environment variables
that aren&amp;#8217;t directly stored in the code. Environment variables can be
set in Civis container scripts as &amp;#8220;Parameters.&amp;#8221; Parameters can be set
to credentials that are saved separately at a User level in&amp;nbsp;Civis.&lt;/p&gt;
&lt;p&gt;Credentials saved in Civis can &lt;span class="caps"&gt;ONLY&lt;/span&gt; be made availabe in the
environment using a &lt;code&gt;_USERNAME&lt;/code&gt; and &lt;code&gt;_PASSWORD&lt;/code&gt; suffix. That is, if
your credential is named &lt;code&gt;SLACK_API_KEY&lt;/code&gt;, Civis forces you to make
secret values available in the environment using keys
&lt;code&gt;SLACK_API_KEY_USERNAME&lt;/code&gt; and &lt;code&gt;SLACK_API_KEY_PASSWORD&lt;/code&gt;. This choice clearly
doesn&amp;#8217;t always make sense and requires a workaround for every
credential that does not fit this&amp;nbsp;pattern.&lt;/p&gt;
&lt;p&gt;Setting up civis scripts with parameters and credentials is a somewhat
tedious manual process. The Civis &lt;span class="caps"&gt;API&lt;/span&gt; does make some, but not all, of
this process&amp;nbsp;scriptable.&lt;/p&gt;
&lt;h2&gt;Programmatic configuration of&amp;nbsp;orchestration&lt;/h2&gt;
&lt;p&gt;Often data pipelines will have some non-trivial logic for how and when
different steps should be triggered, which steps are dependent on
others to complete first, etc. It is possible to set up these kinds of
dependencies and automated scheduling in Civis using the web
app. Automated scheduling is also possible to set up using the &lt;span class="caps"&gt;API&lt;/span&gt;,
however as far as I can tell, it is not possible programmatically
configure dependent workflows outside of the &lt;span class="caps"&gt;GUI&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Orchestration logic reflects important technical and business
considerations and should not be exclusively configured in a
web app. Having a web-app-first or web-app-exclusive interface for
configuring orchestration means that changes to the configuration
can happen without anyone noticing, and reverting changes can be
difficult. Orchestration logic should be explicitly configured in the
same version-controlled code repository that the code itself lives&amp;nbsp;in.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Civis is a tool designed for ease of use. It abstracts away most of
the data engineering otherwise required for analytics work so that
data teams can get up and running quickly and easily with minimal
technical hurdles. This is essential for small, short-lived,
fast-moving campaigns common across the political&amp;nbsp;space.&lt;/p&gt;
&lt;p&gt;This design comes with a fatal flaw, which is that technical staff
using Civis ultimately are &amp;#8220;protected&amp;#8221; from ever needing to learn data
engineering practices. Data staff in the progressive political space
can go years, from team to team, working exclusively in Civis and
never learn data engineering best practices or even fundamental data
engineering concepts. Ultimately, the technical capacity of the
progressive movement depends on technical staff working closer to and
more directly with their data&amp;nbsp;infrastructure.&lt;/p&gt;</content><category term="sharing"></category></entry><entry><title>(Data) Engineering Resources</title><link href="https://austinweisgrau.github.io/data-engineering-resources.html" rel="alternate"></link><published>2023-02-07T00:00:00-08:00</published><updated>2023-02-07T00:00:00-08:00</updated><author><name>Austin Weisgrau</name></author><id>tag:austinweisgrau.github.io,2023-02-07:/data-engineering-resources.html</id><summary type="html">&lt;p&gt;Here is a collection of resources that I&amp;#8217;ve found useful in my
development as a data engineer. I&amp;#8217;ll add to this page over&amp;nbsp;time.&lt;/p&gt;
&lt;h1&gt;Books&lt;/h1&gt;
&lt;h3&gt;&lt;a href="https://www.oreilly.com/library/view/data-pipelines-pocket/9781492087823/"&gt;Data Pipelines Pocket Reference,&amp;nbsp;Densmore&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The best reference book I&amp;#8217;ve come across for the full picture of the modern data engineering toolkit …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here is a collection of resources that I&amp;#8217;ve found useful in my
development as a data engineer. I&amp;#8217;ll add to this page over&amp;nbsp;time.&lt;/p&gt;
&lt;h1&gt;Books&lt;/h1&gt;
&lt;h3&gt;&lt;a href="https://www.oreilly.com/library/view/data-pipelines-pocket/9781492087823/"&gt;Data Pipelines Pocket Reference,&amp;nbsp;Densmore&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The best reference book I&amp;#8217;ve come across for the full picture of the modern data engineering toolkit and workflow. Has lots of useful example&amp;nbsp;code.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://martinfowler.com/books/refactoring.html"&gt;Refactoring, Martin&amp;nbsp;Fowler&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This book completely transformed my relationship to legacy code, and data engineers usually work with a lot of legacy code. If you can compose many trivial, behavior-preserving changes into coherently redesigned and simplified code, you will have no&amp;nbsp;fear.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://jakevdp.github.io/PythonDataScienceHandbook/"&gt;Python Data Science Handbook,&amp;nbsp;VanderPlas&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I especially appreciated its first chapter about&amp;nbsp;ipython.&lt;/p&gt;
&lt;h1&gt;Articles&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://grugbrain.dev/"&gt;The Grug Brained&amp;nbsp;Developer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;learn from many, many mistake grug make over long program&amp;nbsp;life&amp;#8221;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://catb.org/~esr/faqs/smart-questions.html"&gt;How to Ask Questions the Smart&amp;nbsp;Way&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Effective engineers are highly self-sufficient. You will answer 90% of your own questions before you have finished reformulating them as smart&amp;nbsp;questions.&lt;/p&gt;
&lt;h1&gt;Tutorials&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://learnpythonthehardway.org/python3/"&gt;Learn Python the Hard&amp;nbsp;Way&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is where I started. I am a big fan of the &amp;#8220;hard way&amp;#8221; learning philosophy. The main idea is to never read about how to do something without actually doing it yourself as you&amp;nbsp;read.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world"&gt;The Flask&amp;nbsp;Mega-Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial carried me through my first year of work. Not necessarily relevant for data engineering, but if you want to know how to throw together a web application in python, this tutorial is&amp;nbsp;excellent.&lt;/p&gt;</content><category term="learning"></category></entry></feed>