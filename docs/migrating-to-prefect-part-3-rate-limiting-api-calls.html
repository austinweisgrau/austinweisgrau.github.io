<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Migrating to Prefect, Part 3: Rate limiting API calls &mdash; Data Engineering the Left</title>
  <meta name="author" content="Austin Weisgrau">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="https://austinweisgrau.github.io/favicon.png" rel="icon">

  <link href="https://austinweisgrau.github.io/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="https://austinweisgrau.github.io/">Data Engineering the Left</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>


<ul class="main-navigation">
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Migrating to Prefect, Part 3: Rate limiting API calls</h1>
    <p class="meta">
<time datetime="2023-03-02T00:00:00-08:00" pubdate>Thu 02 March 2023</time>    </p>
</header>

  <div class="entry-content"><p><em>This post is the third in a series about migrating off of Civis and
onto Prefect as our orchestration tool. The <a href="https://austinweisgrau.github.io/migrating-to-prefect-part-1-civis-woes.html">first post</a> is about the
limitations of Civis, and the <a href="https://austinweisgrau.github.io/migrating-to-prefect-part-2-prefect-aws-ecs.html">second post</a> is about setting up Prefect
with <span class="caps">AWS</span> <span class="caps">ECS</span>.</em></p>
<p>A common, basic workflow in a data pipeline is to make concurrent,
rate-limited <span class="caps">API</span> calls to fetch or post data to a service.
One obstacle I hit early on when attempting to move our data pipelines
into Prefect was around setting up rate-limited <span class="caps">API</span>&nbsp;calls.</p>
<p><a href="#solution">Skip to solution</a> or see <a href="https://github.com/austinweisgrau/prefect-ecs-template/blob/main/flows/api_call_demonstration/api_call_flow.py">a working example in my prefect template&nbsp;repository</a></p>
<h2>Implementing rate-limited <span class="caps">API</span> calls in standard&nbsp;python</h2>
<p><a href="https://realpython.com/python-concurrency/#when-is-concurrency-useful">Python has two paradigms for executing <span class="caps">IO</span>-bound concurrency</a>: asyncio
and&nbsp;multithreading.</p>
<p>asyncio is a very powerful framework, but involves a somewhat
substantial change to python patterns. It cannot be easily added to
existing code without rewriting the entire codebase for compatibility
with asyncio. It also represents a technical hurdle and maintenance
burden on smaller teams. For a simple workflow like rate-limited <span class="caps">API</span>
calls, using asyncio would be significant&nbsp;overkill.</p>
<p>Multithreading can be complicated to set up, but the python standard
library includes a high-level <span class="caps">API</span> to simplify the use of
multithreading: <code>concurrency.futures</code>. I like to use the
<code>ThreadPoolExecutor</code>, which can be easily configured to limit the
number of active&nbsp;threads. </p>
<p>A standard implementation of concurrent rate-limited <span class="caps">API</span> calls in
python can look like&nbsp;this:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">concurrent.futures</span> <span class="kn">import</span> <span class="n">ThreadPoolExecutor</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="k">def</span> <span class="nf">make_api_call</span><span class="p">(</span><span class="n">payload</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">requests</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Response</span><span class="p">:</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">api_url_endpoint</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>

<span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
    <span class="n">payloads</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">responses</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">make_api_call</span><span class="p">,</span> <span class="n">payloads</span><span class="p">)</span>
</code></pre></div>

<p>This code will execute an <span class="caps">API</span> call on each payload in the list of
payloads, with 3 threads running simultaneously, each executing one
<span class="caps">API</span> call at a time. If each call takes 1 full second, executing 30
calls will take about 10 seconds to&nbsp;complete.</p>
<p>This represents a speed increase of 3x over using a for loop, which
would take a full 30 seconds to&nbsp;complete. </p>
<div class="highlight"><pre><span></span><code><span class="n">responses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">payload</span> <span class="ow">in</span> <span class="n">payloads</span><span class="p">:</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">make_api_call</span><span class="p">(</span><span class="n">payload</span><span class="p">)</span>
    <span class="n">responses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>

<h2>Implementing concurrency in&nbsp;Prefect</h2>
<p>Prefect has several different configurable mechanisms for
orchestrating concurrent execution of python&nbsp;code.</p>
<p>Prefect is built on top of an asyncio implementation called AnyIO and
can seamlessly work with python scripts written using asyncio. For
reasons described above, I don&#8217;t want to use&nbsp;asyncio.</p>
<p>Prefect tasks are the main building block of flows in Prefect, and are
also the main mechanism for orchestrating concurrency. Prefect tasks
called with <code>task.submit()</code> or <code>task.map()</code> are sent to a <a href="https://docs.prefect.io/concepts/task-runners/">Task Runner</a>
for (potentially) concurrent execution. Sequential, concurrent or parallel
execution will occur depending on which task runner is used. The
default task runner is a&nbsp;ConcurrentTaskRunner.</p>
<p>Tasks submitted to a concurrent runner will all execute as soon as
they are not waiting for any upstream&nbsp;values.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">prefect</span> <span class="kn">import</span> <span class="n">flow</span><span class="p">,</span> <span class="n">task</span>

<span class="nd">@task</span>
<span class="k">def</span> <span class="nf">make_api_call</span><span class="p">(</span><span class="n">payload</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">requests</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Response</span><span class="p">:</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">api_url_endpoint</span><span class="p">,</span> <span class="n">payload</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>

<span class="nd">@flow</span>
<span class="k">def</span> <span class="nf">my_flow</span><span class="p">():</span>
    <span class="n">payloads</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">responses</span> <span class="o">=</span> <span class="n">make_api_call</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">payloads</span><span class="p">)</span>
</code></pre></div>

<p>Using prefect makes concurrent execution of python code wonderfully&nbsp;simple.</p>
<p>In the above implementation, if each <span class="caps">API</span> call takes 1 second to
complete and there are 30 total <span class="caps">API</span> calls, the full flow will be done
in about 1 second, because all the calls execute&nbsp;simultaneously.</p>
<h2>Rate limiting concurrency in&nbsp;Prefect</h2>
<p>We still need to implement rate limiting on our <span class="caps">API</span> calls in
Prefect. The code above will execute every <span class="caps">API</span> call across all the
payloads simultaneously. This will generally get your <span class="caps">API</span> calls
temporarily or permanently&nbsp;blocked.</p>
<p>Prefect offers a native solution for limiting concurrency on tasks:
simply enough, a <a href="https://docs.prefect.io/concepts/tasks/#task-run-concurrency-limits">task run concurrency limit</a>. It is simple to set up and
use. Concurrency limit values must be deployed to the server&nbsp;with</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>prefect<span class="w"> </span>concurrency-limit<span class="w"> </span>create<span class="w"> </span>limit_concurrency_10<span class="w"> </span><span class="m">10</span>
</code></pre></div>

<p>And then prefect tasks can be rate limited by tagging them with the
name of the concurrency&nbsp;limit:</p>
<div class="highlight"><pre><span></span><code><span class="nd">@task</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s2">&quot;limit_concurrency_10&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">make_api_call</span><span class="p">(</span><span class="n">payload</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
   <span class="k">pass</span>
</code></pre></div>

<p>However, rather than limiting the number of tasks that can run at any
given moment, this mechanism limits the number of tasks that can run
in any given 30 second block. Prefect tasks with a rate limit check
for an open slot, and if none is available, wait 30 seconds to check
again for an open&nbsp;slot. </p>
<p>This is quite problematic, as it results in very significant slowdowns
of rate-limited execution over the standard python use of
multithreading. If you only want 3 calls at a time, you must set up a
concurrency limit of 3. However, after those 3 calls execute, no more
calls will begin to execute until that initial 30 second window has
closed. If each call takes 1 full second, running 30 calls will take
15 minutes to&nbsp;complete!</p>
<p>The Prefect team is aware of this issue and is working on an improved
implementation of the task-concurrency feature. See <a href="https://github.com/PrefectHQ/prefect/issues/8873">this github issue</a>,
<a href="https://github.com/PrefectHQ/prefect/pull/7013">this github <span class="caps">PR</span></a>, and this <a href="https://prefect-community.slack.com/archives/C03D12VV4NN/p1677533662427229">slack discussion</a>.</p>
<h2>Solution</h2>
<p>One simple solution would be to use normal python mulithreading code
within a prefect task. In Prefect, however, a Prefect task is intended
to be the smallest unit of concurrency. It breaks intended Prefect
patterns to use multithreading from within a prefect task. Some Prefect
utilities are incompatible with multithreaded task code. <a href="https://github.com/PrefectHQ/prefect/issues/8652"><em>See discussion&nbsp;here.</em></a></p>
<p>In the end, the approach I settled on with some help from the Prefect
development team involved using a python <a href="https://superfastpython.com/thread-semaphore/"><code>threading.Semaphore</code></a>. A
Semaphore is a standard approach for rate limiting multithreaded python
code, used behind the scenes by high-level APIs like
<code>concurrent.futures.ThreadPoolExecutor</code>. Each thread attempts to access
an open &#8220;slot&#8221; from the Semaphore, and waits until a slot is available
to run. While it runs, it holds a slot, and releases that slot once it
is&nbsp;complete.</p>
<p>When Prefect tasks are submitted to a ConcurrentTaskRunner, they are
executed in the same python process using multithreading behind the
scenes, which means using a Semaphore is a natural&nbsp;solution.</p>
<p>My implementation of the prefect-compatible semaphore-based
concurrency rate limiting can be found below or in my template
repository <a href="https://github.com/austinweisgrau/prefect-ecs-template/blob/main/utilities/concurrency.py">here</a>, and its use ends up being very&nbsp;simple:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">prefect</span> <span class="kn">import</span> <span class="n">flow</span><span class="p">,</span> <span class="n">task</span>
<span class="kn">from</span> <span class="nn">utilities.concurrency</span> <span class="kn">import</span> <span class="n">limit_concurrency</span>

<span class="nd">@task</span>
<span class="nd">@limit_concurrency</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">make_api_call</span><span class="p">(</span><span class="n">payload</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">requests</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Response</span><span class="p">:</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">api_url_endpoint</span><span class="p">,</span> <span class="n">payload</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>

<span class="nd">@flow</span>
<span class="k">def</span> <span class="nf">my_flow</span><span class="p">():</span>
    <span class="n">payloads</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">responses</span> <span class="o">=</span> <span class="n">make_api_call</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">payloads</span><span class="p">)</span>
</code></pre></div>

<p>Identical to the implementation with <code>ThreadPoolExecutor</code> above,
executing 30 <span class="caps">API</span> calls that take 1 second each will take about 10
seconds with this&nbsp;implementation.</p>
<h2><a id="solution"></a>My concurrency limiting task&nbsp;decorator</h2>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Semaphore</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span>


<span class="k">def</span> <span class="nf">limit_concurrency</span><span class="p">(</span><span class="n">max_workers</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Callable</span><span class="p">],</span> <span class="n">Callable</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wraps methods to implement concurrency limit</span>

<span class="sd">    Prefect task concurrency limits use a 30 second delay between each</span>
<span class="sd">    check for an available slot. This is a more performative approach</span>
<span class="sd">    using a threading.Semaphore.</span>

<span class="sd">    Prefect must be using a &quot;local&quot; task runner for this to work (the</span>
<span class="sd">    ConcurrentTaskRunner) and not a distributed task runner like Dask</span>
<span class="sd">    or Ray.</span>

<span class="sd">    Usage:</span>
<span class="sd">      from prefect import task</span>

<span class="sd">      @task</span>
<span class="sd">      @limit_concurrency(max_workers=5)</span>
<span class="sd">      def my_task():</span>
<span class="sd">          pass</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">semaphore</span> <span class="o">=</span> <span class="n">Semaphore</span><span class="p">(</span><span class="n">max_workers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pseudo_decorator</span><span class="p">(</span><span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
        <span class="nd">@wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">limited_concurrent_func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">semaphore</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">limited_concurrent_func</span>

    <span class="k">return</span> <span class="n">pseudo_decorator</span>
</code></pre></div></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Austin Weisgrau
    </span>
  </span>
<time datetime="2023-03-02T00:00:00-08:00" pubdate>Thu 02 March 2023</time>  <span class="categories">
    <a class='category' href='https://austinweisgrau.github.io/category/sharing.html'>sharing</a>
  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="https://austinweisgrau.github.io/tooling-for-big-data-transformations.html">Tooling for Big Data Transformations</a>
      </li>
      <li class="post">
          <a href="https://austinweisgrau.github.io/deduplicating-everyaction.html">Deduplicating EveryAction</a>
      </li>
      <li class="post">
          <a href="https://austinweisgrau.github.io/migrating-from-civis-to-prefect.html">Migrating from Civis to Prefect</a>
      </li>
      <li class="post">
          <a href="https://austinweisgrau.github.io/migrating-to-prefect-part-4-moving-a-script-from-civis-to-prefect.html">Migrating to Prefect, Part 4: Moving a Script from Civis to Prefect</a>
      </li>
      <li class="post">
          <a href="https://austinweisgrau.github.io/migrating-to-prefect-part-2-prefect-aws-ecs.html">Migrating to Prefect, Part 2: Prefect & AWS ECS</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="https://austinweisgrau.github.io/category/learning.html">learning</a></li>
        <li><a href="https://austinweisgrau.github.io/category/sharing.html">sharing</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/in/austin-weisgrau-a784b042/" target="_blank">LinkedIn</a></li>
            <li><a href="https://github.com/austinweisgrau" target="_blank">GitHub</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2023&ndash;2024  Austin Weisgrau &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="https://austinweisgrau.github.io/theme/js/modernizr-2.0.js"></script>
  <script src="https://austinweisgrau.github.io/theme/js/ender.js"></script>
  <script src="https://austinweisgrau.github.io/theme/js/octopress.js" type="text/javascript"></script>
</body>
</html>